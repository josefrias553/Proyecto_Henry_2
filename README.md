# üèóÔ∏è Proyecto Data Warehouse E-Commerce (M2)

## Visi√≥n General
Este proyecto representa una soluci√≥n de Data Warehouse robusta y escalable dise√±ada para una plataforma de E-Commerce de alto volumen. Implementa un pipeline ELT (Extract, Load, Transform) completo utilizando **Python** para la ingesta de datos y **dbt (data build tool)** para el modelado dimensional y las transformaciones. La arquitectura sigue las mejores pr√°cticas en Ingenier√≠a de Datos, enfoc√°ndose en la modularidad, la calidad de los datos y la reproducibilidad.

## üìê Arquitectura
El sistema est√° dise√±ado con una arquitectura en capas:

1.  **Capa de Ingesta (EL)**: Los loaders basados en Python extraen datos de fuentes crudas (CSV/JSON/API) y los cargan en el Operational Data Store (ODS) de **PostgreSQL**. Esto asegura que se mantenga estrictamente una copia cruda de los datos.
2.  **Capa de Transformaci√≥n (T)**: `dbt` gestiona el ciclo de vida de la transformaci√≥n, promoviendo los datos a trav√©s de tres capas clave:
    *   **Staging**: Vista 1:1 de las tablas fuente con conversi√≥n de tipos, renombrado y limpieza ligera.
    *   **Intermediate**: Joins l√≥gicos, limpieza compleja y aplicaci√≥n de l√≥gica de negocio.
    *   **Mart**: Modelos dimensionales finales (Esquema Estrella) optimizados para herramientas de BI y an√°lisis OLAP (tablas `dim_*` y `fact_*`).

## üóÑÔ∏è Arquitectura de Base de Datos

El proyecto utiliza **PostgreSQL** con una arquitectura de **dos esquemas** para separar las operaciones transaccionales (OLTP) de las anal√≠ticas (OLAP):

### **Esquema `public` - OLTP (Operational Database)**
**Prop√≥sito**: Base de datos operacional que soporta las transacciones del e-commerce en tiempo real.

**Tablas**:
- `usuarios` - Informaci√≥n de clientes registrados
- `productos` - Cat√°logo de productos
- `categorias` - Categor√≠as de productos
- `ordenes` - √ìrdenes de compra
- `detalleordenes` - L√≠neas de pedido (√≠tems por orden)
- `direccionesenvio` - Direcciones de env√≠o de usuarios
- `metodospago` - M√©todos de pago disponibles
- `ordenesmetodospago` - Relaci√≥n orden-m√©todo de pago
- `historialpagos` - Historial de transacciones de pago
- `resenasproductos` - Rese√±as y calificaciones de productos
- `carrito` - Carrito de compras activo

**Caracter√≠sticas**:
- Normalizado (3NF) para evitar redundancia
- Optimizado para operaciones CRUD (Create, Read, Update, Delete)
- Alta frecuencia de escrituras y actualizaciones
- Cargado por Python loaders desde archivos CSV/JSON

### **Esquema `dw` - OLAP (Data Warehouse)**
**Prop√≥sito**: Data Warehouse optimizado para an√°lisis, reporter√≠a y business intelligence.

**Dimensiones** (Tablas `dim_*`):
- `dim_time` - Calendario con atributos fiscales y temporales
- `dim_customer` - Clientes con historial (SCD2)
- `dim_customer_segment` - Segmentaci√≥n de clientes
- `dim_product` - Productos con historial de cambios (SCD2)
- `dim_category` - Categor√≠as de productos
- `dim_address` - Direcciones con historial (SCD2)
- `dim_payment_method` - M√©todos de pago
- `dim_order_status` - Estados de √≥rdenes
- `dim_review` - Rese√±as de productos

**Hechos** (Tablas `fact_*`):
- `fact_order` - √ìrdenes (nivel orden)
- `fact_order_line` - L√≠neas de pedido (nivel √≠tem)
- `fact_payment` - Pagos y transacciones
- `fact_inventory_snapshot` - Snapshots diarios de inventario
- `fact_order_accum` - Ciclo de vida de √≥rdenes (accumulating snapshot)
- `fact_ventas_agg_daily` - Agregados diarios de ventas

**Caracter√≠sticas**:
- Desnormalizado (Star Schema) para consultas r√°pidas
- Historial completo con SCD2 en dimensiones cr√≠ticas
- Generado y mantenido por transformaciones dbt
- Optimizado para agregaciones y an√°lisis multidimensional

### **Flujo de Datos**
```
Fuentes Raw (CSV/JSON)
        ‚Üì
[Python Loaders] ‚Üí Esquema 'public' (ODS)
        ‚Üì
[dbt - Staging] ‚Üí Limpieza y normalizaci√≥n
        ‚Üì
[dbt - Intermediate] ‚Üí L√≥gica de negocio
        ‚Üì
[dbt - Mart] ‚Üí Esquema 'dw' (Star Schema)
        ‚Üì
Herramientas BI (PowerBI, Tableau, Metabase)
```

## üõ†Ô∏è Tech Stack
*   **Orquestaci√≥n y Scripting**: Python 3.8+
*   **Data Warehouse / Base de Datos**: PostgreSQL
*   **Transformaci√≥n**: dbt Core
*   **Calidad y Testing**: dbt Tests, Python `unittest`

## üöÄ Gu√≠a de Inicio

### Requisitos Previos
*   Python 3.8 o superior
*   Instancia de PostgreSQL en ejecuci√≥n y accesible
*   Conocimiento b√°sico de SQL y herramientas CLI

### Instalaci√≥n

1.  **Clonar el repositorio**:
    ```bash
    git clone <url-del-repositorio>
    cd Proyecto_Henry_2_V1
    ```

2.  **Configurar el entorno**:
    ```bash
    python -m venv .venv
    # Windows
    .venv\Scripts\activate
    # Unix/MacOS
    source .venv/bin/activate
    ```

3.  **Instalar dependencias**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configuraci√≥n**:
    *   Copiar `.env.example` a `.env`.
    *   Actualizar `.env` con tus credenciales de PostgreSQL.
    *   Asegurar que tu `profiles.yml` para dbt est√© configurado correctamente (apuntado por `dbt_project.yml`).

## ‚öôÔ∏è Uso

El proyecto utiliza un punto de entrada CLI unificado `main.py` para todas las tareas de ingesta.

### Ingesta de Datos (Loaders)
Inicializar el esquema de la base de datos y cargar todos los datasets:
```bash
python main.py --init-db --load all
```

Cargar una entidad espec√≠fica (ej., solo usuarios):
```bash
python main.py --load usuarios
```

### Transformaciones dbt

Ejecutar todos los modelos dbt (staging ‚Üí intermediate ‚Üí mart):
```bash
dbt run
```

Ejecutar solo una capa espec√≠fica:
```bash
# Solo staging
dbt run --select staging

# Solo intermediate
dbt run --select intermediate

# Solo mart (dimensiones y hechos)
dbt run --select mart
```

Ejecutar un modelo espec√≠fico:
```bash
dbt run --select dim_customer
dbt run --select fact_order_line
```

Ejecutar pruebas de calidad de datos:
```bash
dbt test
```

Generar documentaci√≥n:
```bash
dbt docs generate
dbt docs serve
```

### Validaci√≥n para GitHub

Antes de subir el proyecto a GitHub, verificar que los archivos sensibles est√©n excluidos:
```bash
# Verificar estado de git
git status

# Los siguientes archivos NO deben aparecer:
# - .env (credenciales)
# - __pycache__/ (cache de Python)
# - .venv/ (entorno virtual)
# - target/ (artifacts de dbt)
# - dbt_packages/ (paquetes descargados)
# - logs/ y *.log (archivos de log)
```

## üìÇ Estructura del Proyecto

```
‚îú‚îÄ‚îÄ analysis/               # Scripts de an√°lisis ad-hoc y chequeos de calidad
‚îú‚îÄ‚îÄ data/                   # Almacenamiento de datos crudos
‚îú‚îÄ‚îÄ dbt_ecommerce_dw/       # Modelos dbt
‚îÇ   ‚îî‚îÄ‚îÄ models/             # L√≥gica SQL para Staging, Intermediate y Marts
‚îú‚îÄ‚îÄ dbt_project.yml         # Configuraci√≥n de dbt
‚îú‚îÄ‚îÄ loaders/                # Scripts Python para ingesta de datos (Extract/Load)
‚îú‚îÄ‚îÄ docs/                   # Activos de documentaci√≥n
‚îú‚îÄ‚îÄ SQL/                    # Scripts SQL auxiliares
‚îú‚îÄ‚îÄ tests/                  # Tests unitarios Python y Tests singulares dbt
‚îú‚îÄ‚îÄ main.py                 # Punto de entrada CLI
‚îú‚îÄ‚îÄ models.py               # Definiciones ORM de SQLAlchemy
‚îú‚îÄ‚îÄ db.py                   # Gestor de conexi√≥n a base de datos
‚îî‚îÄ‚îÄ utils.py                # Funciones auxiliares
```

## üìä Estrategia de Modelado de Datos

El Data Warehouse est√° construido bajo un enfoque de **Esquema Estrella**:

*   **Tablas de Hechos (Fact)**: Capturan procesos de negocio (ej., `fact_orders`, `fact_sales`).
*   **Tablas de Dimensiones (Dimension)**: Proveen contexto (ej., `dim_product`, `dim_customer`, `dim_time`).

Esta estructura asegura un alto rendimiento para consultas anal√≠ticas e integraci√≥n fluida con herramientas de BI como PowerBI, Tableau o Metabase.



